{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/riccardoconci/anaconda3/envs/compneuro/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/riccardoconci/anaconda3/envs/compneuro/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/riccardoconci/anaconda3/envs/compneuro/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.3527 - val_loss: 0.2713\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.2639 - val_loss: 0.2526\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.2425 - val_loss: 0.2300\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2221 - val_loss: 0.2119\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.2068 - val_loss: 0.1991\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1957 - val_loss: 0.1894\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1871 - val_loss: 0.1818\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1803 - val_loss: 0.1757\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.1745 - val_loss: 0.1703\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.1694 - val_loss: 0.1653\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1647 - val_loss: 0.1609\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1603 - val_loss: 0.1566\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1563 - val_loss: 0.1528\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.1526 - val_loss: 0.1493\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1492 - val_loss: 0.1461\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1461 - val_loss: 0.1430\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1432 - val_loss: 0.1403\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1405 - val_loss: 0.1375\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1379 - val_loss: 0.1352\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1355 - val_loss: 0.1329\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.1333 - val_loss: 0.1306\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1311 - val_loss: 0.1285\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.1291 - val_loss: 0.1266\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.1271 - val_loss: 0.1246\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.1252 - val_loss: 0.1228\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1234 - val_loss: 0.1211\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1217 - val_loss: 0.1193\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 87us/step - loss: 0.1201 - val_loss: 0.1178\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.1186 - val_loss: 0.1163\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.1172 - val_loss: 0.1149\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1158 - val_loss: 0.1136\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1146 - val_loss: 0.1124\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1134 - val_loss: 0.1113\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.1123 - val_loss: 0.1102\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.1113 - val_loss: 0.1093\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.1104 - val_loss: 0.1084\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1095 - val_loss: 0.1075\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 89us/step - loss: 0.1087 - val_loss: 0.1068\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.1080 - val_loss: 0.1060\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.1073 - val_loss: 0.1053\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1066 - val_loss: 0.1047\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1060 - val_loss: 0.1041\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1054 - val_loss: 0.1036\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.1049 - val_loss: 0.1031\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1044 - val_loss: 0.1026\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.1039 - val_loss: 0.1021\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.1035 - val_loss: 0.1017\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1030 - val_loss: 0.1013\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1026 - val_loss: 0.1009\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1023 - val_loss: 0.1005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d99c9e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a sparsity constraint on the encoded representations\n",
    "In the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would \"fire\" at a given time. In Keras, this can be done by adding an activity_regularizer to our Dense layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "# add a Dense layer with a L1 activity regularizer\n",
    "encoded = Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.6730 - val_loss: 0.6484\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.6284 - val_loss: 0.6090\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.5916 - val_loss: 0.5749\n",
      "Epoch 4/100\n",
      "28928/60000 [=============>................] - ETA: 2s - loss: 0.5675"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0ac457a370aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 validation_data=(x_test, x_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep autoencoder\n",
    "We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoder\n",
    "Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train it, we will use the original MNIST digits with shape (samples, 3, 28, 28), and we will just normalize pixel values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this model for 50 epochs. For the sake of demonstrating how to visualize the results of a model during training, we will be using the TensorFlow backend and the TensorBoard callback.\n",
    "\n",
    "First, let's open up a terminal and start a TensorBoard server that will read logs stored at /tmp/autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=/tmp/autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's train our model. In the callbacks list we pass an instance of the TensorBoard callback. After every epoch, this callback will write logs to /tmp/autoencoder, which can be read by our TensorBoard server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.2156 - val_loss: 0.1842\n",
      "Epoch 2/50\n",
      "36224/60000 [=================>............] - ETA: 32s - loss: 0.1639"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3b6b3754a0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compneuro/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence autoencoder\n",
    "If you inputs are sequences, rather than vectors or 2D images, then you may want to use as encoder and decoder a type of model that can capture temporal structure, such as a LSTM. To build a LSTM-based autoencoder, first use a LSTM encoder to turn your input sequences into a single vector that contains information about the entire sequence, then repeat this vector n times (where n is the number of timesteps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.\n",
    "\n",
    "We won't be demonstrating that one on any specific dataset. We will just put a code example here for future reference for the reader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder (VAE)\n",
    "Variational autoencoders are a slightly more modern and interesting take on autoencoding.\n",
    "\n",
    "What is a variational autoencoder, you ask? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\".\n",
    "\n",
    "How does a variational autoencoder work?\n",
    "\n",
    "First, an encoder network turns the input samples x into two parameters in a latent space, which we will note z_mean and z_log_sigma. Then, we randomly sample similar points z from the latent normal distribution that is assumed to generate the data, via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n",
    "\n",
    "The parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.\n",
    "\n",
    "Because a VAE is a more complex example, we have made the code available on Github as a standalone script. Here we will review step by step how the model is created.\n",
    "\n",
    "First, here's our encoder network, mapping inputs to our latent distribution parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9e785c714a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mz_log_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
